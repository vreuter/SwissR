% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/info-theory.R
\name{shannonEntropy}
\alias{shannonEntropy}
\title{Calculator of Shannon entropy for a vector of observations.}
\usage{
shannonEntropy(observations, normalized = FALSE)
}
\arguments{
\item{observations}{The observed values for which to calculate entropy.}

\item{normalized}{Whether to normalize the entropy (into [0, 1]).}
}
\value{
The Shannon entropy of the empirical distribution defined by 
        the given observations.
}
\description{
\code{shannonEntropy} calculates the Shannon entropy for a vector 
of observations, using empirical probability of each unique value 
as its probability. The input vector is assumed to contain 
observations from a discrete distribution. If working with continuous 
data, the observations should be discretized prior to using this function.
The base two logarithm is used, so the unit for the result is bits.
}
